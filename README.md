
# Проект AIRI Research

## Структура репозитория

- **/main.py**: Главный файл, запускающий обучение и тестирование модели агента.
- **/env.py** — Содержит код среды. Здесь можно найти механику её работы и механику взаимодействия между агентами.
- **/agents.py** — Cодержит код агентов. Здесь можно найти реализацию и параметры Q-learning.
- **/analysis_scripts** — Скрипты для анализа данных. Главный файл - show_table.py. Визуализирует Q-таблицы агентов.
  - `show_table.py` — Скрипт для визуализации Q-таблиц и поведения агентов.
  - `find_point_down_min_epsilon.py` — Калькулятор для анализа значений ε.
- **/img** — Изображения, которые используются в проекте.
- **/visualization.py** — Модуль для создании видео-визуализации.
- **/requirements.txt** — Список зависимостей.

## Установка

1. Клонируйте репозиторий:

```bash
git clone https://github.com/CommissarNeutrino/Enviroment.git
```

2. Настройте виртуальное окружение:

```bash
python3 -m venv venv
source venv/bin/activate  # для Linux/Mac
venv\Scripts\activate     # для Windows
```

3. Установите зависимости из файла `requirements.txt`:

```bash
pip install -r requirements.txt
```

## Запуск

### Обучение агентов

Используйте скрипт `main.py`. Вы можете выбрать сценарий и указать, нужно ли вам обучение нового агента/тестирование одного из обученных ранее.

Пример использования:

```bash
python main.py scenario_num=1a
```

Параметры командной строки:

-- scenario_num=<тип_карты>    Обязательный параметр. Указывает, какой сценарий запускать.
                               Возможные варианты: 1a, 1b, 2a, 2b, 2c, 3a, 3b, 3c, 4a, 4b, 4c и т.д.

-- progon_num=<номер>          Опциональный. Указывает номер ранее сохранённого прогона
                               для загрузки данных при тестировании.

-- no_learn                    Отключает этап обучения. Используется, если необходимо только тестирование.

-- no_test                     Отключает этап тестирования. Используется, если необходимо только обучение.

Примеры запуска:

1. Обучение и тестирование на карте 1a:
   python main.py scenario_num=1a

2. Только обучение (без тестирования):
   python main.py scenario_num=3b no_test

3. Только тестирование ранее обученного агента:
   python main.py scenario_num=3b no_learn progon_num=2

Дополнительные функции (вызываются вручную в коде):

- altruist_horizon_iterator_training():
  Автоматический прогон обучения с разными значениями γ и горизонтом времени
  для сценариев 2c, 3b, 4c.

- altruist_horizon_iterator_testing():
  Тестирование агентов после обучения, по заранее заданным конфигурациям.

### Пример загрузки и построения графиков

Для загрузки данных из файла `.pkl` и анализа Q-таблиц используйте скрипт `analysis_scripts/show_table.py`.
Он загружает сохранённые Q-таблицы, преобразует их в таблицу действий и визуализирует оптимальные направления движения агента.

Пример использования:

```bash
python python show_table.py scenario_num=<номер_сценария> [agent_type=<тип_агента>] [progon_num=<номер_прогона>]
```

Параметры:
    scenario_num  — номер сценария для загрузки (например: 1a, 2b). Обязательно.
    agent_type    — тип агента (по умолчанию 'patron').
    progon_num    — номер прогона для загрузки (по умолчанию берётся последний доступный).
    --help        — вывести это сообщение и завершить выполнение.

Примеры запуска:
    python show_table.py scenario_num=1a
    python show_table.py scenario_num=2b agent_type=altruist progon_num=5

Описание работы:
    1. Скрипт ищет сохранённые данные в папке cache/<сценарий>/progon_<номер>/table_<agent_type>.
    2. Загружает Q-таблицу и преобразует её в читаемый DataFrame.
    3. Определяет лучшее действие для каждого состояния.
    4. Визуализирует карту с указанием направлений оптимальных действий.

### Структура данных

Проект использует Q-таблицы для обучения агентов. Q-таблицы представляют собой структуры данных, которые содержат оптимальные действия для каждого состояния.

## Примечания

- Для получения подробной информации о параметрах запуска используйте флаг `--help`. (Доступен для main.py и analysis_scripts/show_table.py)
- Все данные сохраняются в каталоге `cache`, где каждый сценарий имеет свою структуру папок и файлов.
